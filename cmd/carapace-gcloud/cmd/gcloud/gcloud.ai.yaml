# yaml-language-server: $schema=https://carapace.sh/schemas/command.json
name: ai
description: Manage entities in Vertex AI.
commands:
    - name: custom-jobs
      description: Manage Vertex AI custom jobs.
      commands:
        - name: cancel
          description: Cancel a running custom job.
          flags:
            --region=: Cloud region for the custom job.
        - name: create
          description: Create a new custom job.
          flags:
            --args=: Comma-separated arguments passed to containers or python tasks.
            --command=: Command to be invoked when containers are started.
            --config=: Path to the job configuration file.
            --display-name=!: Display name of the custom job to create.
            --enable-dashboard-access: Whether you want Vertex AI to enable dashboard built on the training containers.
            --enable-web-access: Whether you want Vertex AI to enable [interactive shell access](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell)
            --kms-key=!: ID of the key or fully qualified identifier for the key.
            --kms-keyring=: The KMS keyring of the key.
            --kms-location=: The Google Cloud location for the key.
            --kms-project=: The Google Cloud project for the key.
            --labels=: List of label KEY=VALUE pairs to add.
            --network=: Full name of the Google Compute Engine network to which the Job
            --no-enable-dashboard-access&: Whether you want Vertex AI to enable dashboard built on the training containers.
            --no-enable-web-access&: Whether you want Vertex AI to enable [interactive shell access](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell)
            --persistent-resource-id=: The name of the persistent resource from the same project and region on
            --python-package-uris=: The common Python package URIs to be used for training with a pre-built container image.
            --region=: ID of the region or fully qualified identifier for the region.
            --service-account=: The email address of a service account to use when running the
            --worker-pool-spec=: Define the worker pool configuration used by the custom job.
        - name: describe
          description: Get detailed information about the custom job by given id.
          flags:
            --region=: Cloud region for the custom job.
        - name: list
          description: Lists the existing custom jobs.
          flags:
            --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
            --limit=: Maximum number of resources to list.
            --no-uri&: Print a list of resource URIs instead of the default output, and change the
            --page-size=: Some services group resource list output into pages.
            --region=: ID of the region or fully qualified identifier for the region.
            --sort-by=: Comma-separated list of resource field key names to sort by.
            --uri: Print a list of resource URIs instead of the default output, and change the
        - name: local-run
          description: Run a custom training locally.
          flags:
            --docker-run-options=&: ""
            --executor-image-uri=!: URI or ID of the container image in either the Container Registry or local
            --extra-dirs=: Extra directories under the working directory to include, besides the one
            --extra-packages=: Local paths to Python archives used as training dependencies in the image
            --gpu: Enable to use GPU.
            --local-package-path=: local path of the directory where the python-module or script exists.
            --no-gpu&: Enable to use GPU.
            --output-image-uri=: Uri of the custom container image to be built with the your application
            --python-module=: Name of the python module to execute, in 'trainer.train' or 'train'
            --requirements=: Python dependencies from PyPI to be used when running the application.
            --script=: The relative path of the file to execute.
            --service-account-key-file=: The JSON file of a Google Cloud service account private key.
        - name: stream-logs
          description: Show stream logs from a running custom job.
          flags:
            --allow-multiline-logs: Output multiline log messages as single records.
            --no-allow-multiline-logs&: Output multiline log messages as single records.
            --polling-interval=: Number of seconds to wait between efforts to fetch the latest log messages.
            --region=: Cloud region for the custom job.
            --task-name=: If set, display only the logs for this particular task.
    - name: endpoints
      description: Manage Vertex AI endpoints.
      commands:
        - name: create
          description: Create a new Vertex AI endpoint.
          flags:
            --description=: Description of the endpoint.
            --display-name=!: Display name of the endpoint.
            --encryption-kms-key-name=: The Cloud KMS resource identifier of the customer managed encryption key
            --endpoint-id=: User-specified ID of the endpoint.
            --gdce-zone=&: The name of the GDCE zone.
            --labels=: List of label KEY=VALUE pairs to add.
            --network=: The full name of the Google Compute Engine network to which the endpoint should be peered.
            --region=: ID of the region or fully qualified identifier for the region.
            --request-response-logging-rate=: Prediction request & response sampling rate for logging to BigQuery table.
            --request-response-logging-table=: BigQuery table uri for prediction request & response logging.
        - name: delete
          description: Delete an existing Vertex AI endpoint.
          flags:
            --region=: Cloud region for the endpoint.
        - name: deploy-model
          description: Deploy a model to an existing Vertex AI endpoint.
          flags:
            --accelerator=: Manage the accelerator config for GPU serving.
            --autoscaling-metric-specs=: Metric specifications that control autoscaling behavior.
            --deployed-model-id=: User-specified ID of the deployed-model.
            --disable-container-logging: For custom-trained Models and AutoML Tabular Models, the container of the
            --display-name=!: Display name of the deployed model.
            --enable-access-logging: If true, online prediction access logs are sent to Cloud Logging.
            --machine-type=: The machine resources to be used for each node of this deployment.
            --max-replica-count=: Maximum number of machine replicas for the deployment resources the model will be
            --min-replica-count=: Minimum number of machine replicas for the deployment resources the model will be
            --model=!: ID of the uploaded model.
            --no-disable-container-logging&: For custom-trained Models and AutoML Tabular Models, the container of the
            --no-enable-access-logging&: If true, online prediction access logs are sent to Cloud Logging.
            --no-spot&: If true, schedule the deployment workload on Spot VMs.
            --region=: Cloud region for the endpoint.
            --required-replica-count=: Required number of machine replicas for the deployment resources the model will
            --reservation-affinity=: A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
            --service-account=: Service account that the deployed model's container runs as.
            --spot: If true, schedule the deployment workload on Spot VMs.
            --traffic-split=: List of pairs of deployed model id and value to set as traffic split.
        - name: describe
          description: Describe an existing Vertex AI endpoint.
          flags:
            --region=: Cloud region for the endpoint.
        - name: direct-predict
          description: Run Vertex AI online direct prediction.
          flags:
            --json-request=!: Path to a local file containing the body of a JSON request.
            --region=: Cloud region for the endpoint.
        - name: direct-raw-predict
          description: Run Vertex AI online direct raw prediction.
          flags:
            --json-request=!: Path to a local file containing the body of a JSON request.
            --region=: Cloud region for the endpoint.
        - name: explain
          description: Request an online explanation from an Vertex AI endpoint.
          flags:
            --deployed-model-id=: Id of the deployed model.
            --json-request=!: Path to a local file containing the body of a JSON request.
            --region=: Cloud region for the endpoint.
        - name: list
          description: List existing Vertex AI endpoints.
          flags:
            --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
            --limit=: Maximum number of resources to list.
            --list-model-garden-endpoints-only: Whether to only list endpoints related to Model Garden.
            --no-list-model-garden-endpoints-only&: Whether to only list endpoints related to Model Garden.
            --no-uri&: Print a list of resource URIs instead of the default output, and change the
            --page-size=: Some services group resource list output into pages.
            --region=: ID of the region or fully qualified identifier for the region.
            --sort-by=: Comma-separated list of resource field key names to sort by.
            --uri: Print a list of resource URIs instead of the default output, and change the
        - name: predict
          description: Run Vertex AI online prediction.
          flags:
            --json-request=!: Path to a local file containing the body of a JSON request.
            --region=: Cloud region for the endpoint.
        - name: raw-predict
          description: Run Vertex AI online raw prediction.
          flags:
            --http-headers=: List of header and value pairs to send as part of the request.
            --region=: Cloud region for the endpoint.
            --request=!: The request to send to the endpoint.
        - name: stream-direct-predict
          description: Run Vertex AI online stream direct prediction.
          flags:
            --json-request=!: Path to a local file containing the body of a JSON request.
            --region=: Cloud region for the endpoint.
        - name: stream-direct-raw-predict
          description: Run Vertex AI online stream direct raw prediction.
          flags:
            --json-request=!: Path to a local file containing the body of a JSON request.
            --region=: Cloud region for the endpoint.
        - name: stream-raw-predict
          description: Run Vertex AI online stream raw prediction.
          flags:
            --http-headers=: List of header and value pairs to send as part of the request.
            --region=: Cloud region for the endpoint.
            --request=!: The request to send to the endpoint.
        - name: undeploy-model
          description: Undeploy a model from an existing Vertex AI endpoint.
          flags:
            --deployed-model-id=!: Id of the deployed model.
            --region=: Cloud region for the endpoint.
            --traffic-split=: List of pairs of deployed model id and value to set as traffic split.
        - name: update
          description: Update an existing Vertex AI endpoint.
          flags:
            --clear-labels: Remove all labels.
            --clear-traffic-split: Clears the traffic split map.
            --description=: Description of the endpoint.
            --disable-request-response-logging: Disable prediction request & response logging.
            --display-name=: Display name of the endpoint.
            --no-clear-labels&: Remove all labels.
            --no-clear-traffic-split&: Clears the traffic split map.
            --no-disable-request-response-logging&: Disable prediction request & response logging.
            --region=: Cloud region for the endpoint.
            --remove-labels=: List of label keys to remove.
            --request-response-logging-rate=: Prediction request & response sampling rate for logging to BigQuery table.
            --request-response-logging-table=: BigQuery table uri for prediction request & response logging.
            --traffic-split=: List of pairs of deployed model id and value to set as traffic split.
            --update-labels=: List of label KEY=VALUE pairs to update.
    - name: hp-tuning-jobs
      description: Manage Vertex AI hyperparameter tuning jobs.
      commands:
        - name: cancel
          description: Cancel a running hyperparameter tuning job.
          flags:
            --region=: Cloud region for the hyperparameter tuning job.
        - name: create
          description: Create a hyperparameter tuning job.
          flags:
            --algorithm=: Search algorithm specified for the given study.
            --config=!: Path to the job configuration file.
            --display-name=!: Display name of the hyperparameter tuning job to create.
            --enable-dashboard-access: Whether you want Vertex AI to enable dashboard built on the training containers.
            --enable-web-access: Whether you want Vertex AI to enable [interactive shell access](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell)
            --kms-key=!: ID of the key or fully qualified identifier for the key.
            --kms-keyring=: The KMS keyring of the key.
            --kms-location=: The Google Cloud location for the key.
            --kms-project=: The Google Cloud project for the key.
            --labels=: List of label KEY=VALUE pairs to add.
            --max-trial-count=: Desired total number of trials.
            --network=: Full name of the Google Compute Engine network to which the Job
            --no-enable-dashboard-access&: Whether you want Vertex AI to enable dashboard built on the training containers.
            --no-enable-web-access&: Whether you want Vertex AI to enable [interactive shell access](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell)
            --parallel-trial-count=: Desired number of Trials to run in parallel.
            --region=: ID of the region or fully qualified identifier for the region.
            --service-account=: The email address of a service account to use when running the
          completion:
            flag:
                algorithm:
                    - algorithm-unspecified
                    - grid-search
                    - random-search
        - name: describe
          description: Get detail information about the hyperparameter tuning job by given id.
          flags:
            --region=: Cloud region for the hyperparameter tuning job.
        - name: list
          description: List existing hyperparameter tuning jobs.
          flags:
            --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
            --limit=: Maximum number of resources to list.
            --no-uri&: Print a list of resource URIs instead of the default output, and change the
            --page-size=: Some services group resource list output into pages.
            --region=: ID of the region or fully qualified identifier for the region.
            --sort-by=: Comma-separated list of resource field key names to sort by.
            --uri: Print a list of resource URIs instead of the default output, and change the
        - name: stream-logs
          description: Stream logs from a running Vertex AI hyperparameter tuning job.
          flags:
            --allow-multiline-logs: Output multiline log messages as single records.
            --no-allow-multiline-logs&: Output multiline log messages as single records.
            --polling-interval=: Number of seconds to wait between efforts to fetch the latest log messages.
            --region=: Cloud region for the hyperparameter tuning job.
            --task-name=: If set, display only the logs for this particular task.
    - name: index-endpoints
      description: Manage Vertex AI index endpoints.
      commands:
        - name: create
          description: Create a new Vertex AI index endpoint.
          flags:
            --description=: Description of the index endpoint.
            --display-name=!: Display name of the index endpoint.
            --enable-private-service-connect: If true, expose the index endpoint via private service connect.
            --encryption-kms-key-name=: The Cloud KMS resource identifier of the customer managed encryption key
            --labels=: List of label KEY=VALUE pairs to add.
            --network=: The Google Compute Engine network name to which the IndexEndpoint should be peered.
            --no-enable-private-service-connect&: If true, expose the index endpoint via private service connect.
            --no-public-endpoint-enabled&: ""
            --project-allowlist=: List of projects from which the forwarding rule will target the service
            --public-endpoint-enabled: If true, the deployed index will be accessible through public endpoint.
            --region=: ID of the region or fully qualified identifier for the region.
        - name: delete
          description: Delete an existing Vertex AI index endpoint.
          flags:
            --region=: Cloud region for the index_endpoint.
        - name: deploy-index
          description: Deploy an index to a Vertex AI index endpoint.
          flags:
            --allowed-issuers=: List of allowed JWT issuers for a deployed index.
            --audiences=: List of JWT audiences that are allowed to access a deployed index.
            --deployed-index-id=!: Id of the deployed index.
            --deployment-group=: Deployment group can be no longer than 64 characters (eg:`test`, `prod`).
            --display-name=!: Display name of the deployed index.
            --enable-access-logging: If true, online prediction access logs are sent to Cloud Logging.
            --index=!: ID of the index.
            --machine-type=: The machine resources to be used for each node of this deployment.
            --max-replica-count=: Maximum number of machine replicas the deployed index will be always deployed on.
            --min-replica-count=: Minimum number of machine replicas the deployed index will be always deployed
            --no-enable-access-logging&: If true, online prediction access logs are sent to Cloud Logging.
            --psc-automation-configs=: A pair of `project-id` and `network` the PSC index will be deployed to.
            --region=: Cloud region for the index_endpoint.
            --reserved-ip-ranges=: List of reserved IP ranges deployed index will be deployed to.
        - name: describe
          description: Gets detailed index endpoint information about the given index endpoint id.
          flags:
            --region=: Cloud region for the index_endpoint.
        - name: list
          description: Lists the index endpoints of the given project and region.
          flags:
            --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
            --limit=: Maximum number of resources to list.
            --no-uri&: Print a list of resource URIs instead of the default output, and change the
            --page-size=: Some services group resource list output into pages.
            --region=: ID of the region or fully qualified identifier for the region.
            --sort-by=: Comma-separated list of resource field key names to sort by.
            --uri: Print a list of resource URIs instead of the default output, and change the
        - name: mutate-deployed-index
          description: Mutate an existing deployed index from a Vertex AI index endpoint.
          flags:
            --allowed-issuers=: List of allowed JWT issuers for a deployed index.
            --audiences=: List of JWT audiences that are allowed to access a deployed index.
            --deployed-index-id=!: Id of the deployed index.
            --deployment-group=: Deployment group can be no longer than 64 characters (eg:`test`, `prod`).
            --enable-access-logging: If true, online prediction access logs are sent to Cloud Logging.
            --machine-type=: The machine resources to be used for each node of this deployment.
            --max-replica-count=: Maximum number of machine replicas the deployed index will be always deployed on.
            --min-replica-count=: Minimum number of machine replicas the deployed index will be always deployed
            --no-enable-access-logging&: If true, online prediction access logs are sent to Cloud Logging.
            --region=: Cloud region for the index_endpoint.
            --reserved-ip-ranges=: List of reserved IP ranges deployed index will be deployed to.
        - name: undeploy-index
          description: Undeploy an index from a Vertex AI index endpoint.
          flags:
            --deployed-index-id=!: Id of the deployed index.
            --region=: Cloud region for the index_endpoint.
        - name: update
          description: Update an Vertex AI index endpoint.
          flags:
            --clear-labels: Remove all labels.
            --description=: Description of the index endpoint.
            --display-name=: Display name of the index endpoint.
            --no-clear-labels&: Remove all labels.
            --region=: Cloud region for the index_endpoint.
            --remove-labels=: List of label keys to remove.
            --update-labels=: List of label KEY=VALUE pairs to update.
    - name: indexes
      description: Manage Vertex AI indexes.
      commands:
        - name: create
          description: Create a new Vertex AI index.
          flags:
            --description=: Description of the index.
            --display-name=!: Display name of the index.
            --encryption-kms-key-name=: The Cloud KMS resource identifier of the customer managed encryption key
            --index-update-method=: The update method to use with this index.
            --labels=: List of label KEY=VALUE pairs to add.
            --metadata-file=!: Path to a local JSON file that contains the additional metadata information about the index.
            --metadata-schema-uri=: Points to a YAML file stored on Google Cloud Storage describing additional information about index.
            --region=: ID of the region or fully qualified identifier for the region.
          completion:
            flag:
                index-update-method:
                    - batch-update
                    - stream-update
        - name: delete
          description: Delete an existing Vertex AI index.
          flags:
            --region=: Cloud region for the index.
        - name: describe
          description: Gets detailed index information about the given index id.
          flags:
            --region=: Cloud region for the index.
        - name: list
          description: Lists the indexes of the given project and region.
          flags:
            --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
            --limit=: Maximum number of resources to list.
            --no-uri&: Print a list of resource URIs instead of the default output, and change the
            --page-size=: Some services group resource list output into pages.
            --region=: ID of the region or fully qualified identifier for the region.
            --sort-by=: Comma-separated list of resource field key names to sort by.
            --uri: Print a list of resource URIs instead of the default output, and change the
        - name: remove-datapoints
          description: Remove data points from the specified index.
          flags:
            --datapoint-ids=: List of index datapoint ids to be removed from the index.
            --datapoints-from-file=: Path to a local JSON file that contains the data points that need to be added to the index.
            --region=: Cloud region for the index.
        - name: update
          description: Update an Vertex AI index.
          flags:
            --clear-labels: Remove all labels.
            --description=: Description of the index.
            --display-name=: Display name of the index.
            --metadata-file=: Path to a local JSON file that contains the additional metadata information about the index.
            --no-clear-labels&: Remove all labels.
            --region=: Cloud region for the index.
            --remove-labels=: List of label keys to remove.
            --update-labels=: List of label KEY=VALUE pairs to update.
        - name: upsert-datapoints
          description: Upsert data points into the specified index.
          flags:
            --datapoints-from-file=!: Path to a local JSON file that contains the data points that need to be added to the index.
            --region=: Cloud region for the index.
            --update-mask=: Update mask is used to specify the fields to be
    - name: model-garden
      description: Interact with and manage resources in Vertex Model Garden.
      commands:
        - name: models
          description: List and use Model Garden models.
          commands:
            - name: deploy
              description: Deploy a model in Model Garden to a Vertex AI endpoint.
              flags:
                --accelerator-count=: The accelerator count to serve the model.
                --accelerator-type=: The accelerator type to serve the model.
                --accept-eula: When set, the user accepts the End User License Agreement (EULA) of the model.
                --asynchronous: If set to true, the command will terminate immediately and not keep polling the operation status.
                --container-args=: Comma-separated arguments passed to the command run by the container
                --container-command=: Entrypoint for the container image.
                --container-deployment-timeout-seconds=: Deployment timeout in seconds.
                --container-env-vars=: List of key-value pairs to set as environment variables.
                --container-grpc-ports=: Container ports to receive grpc requests at.
                --container-health-probe-exec=: Exec specifies the action to take.
                --container-health-probe-period-seconds=: How often (in seconds) to perform the health probe.
                --container-health-probe-timeout-seconds=: Number of seconds after which the health probe times out.
                --container-health-route=: HTTP path to send health checks to inside the container.
                --container-image-uri=: URI of the Model serving container file in the Container Registry
                --container-ports=: Container ports to receive http requests at.
                --container-predict-route=: HTTP path to send prediction requests to inside the container.
                --container-shared-memory-size-mb=: The amount of the VM memory to reserve as the shared memory for the model in
                --container-startup-probe-exec=: Exec specifies the action to take.
                --container-startup-probe-period-seconds=: How often (in seconds) to perform the startup probe.
                --container-startup-probe-timeout-seconds=: Number of seconds after which the startup probe times out.
                --enable-fast-tryout: If True, model will be deployed using faster deployment path.
                --endpoint-display-name=: Display name of the endpoint with the deployed model.
                --hugging-face-access-token=: The access token from Hugging Face needed to read the model artifacts of gated models.
                --machine-type=: The machine type to deploy the model to.
                --model=!: The model to be deployed.
                --no-accept-eula&: When set, the user accepts the End User License Agreement (EULA) of the model.
                --no-asynchronous&: If set to true, the command will terminate immediately and not keep polling the operation status.
                --no-enable-fast-tryout&: If True, model will be deployed using faster deployment path.
                --no-spot&: If true, schedule the deployment workload on Spot VM.
                --no-use-dedicated-endpoint&: If true, the endpoint will be exposed through a dedicated DNS.
                --region=: ID of the region or fully qualified identifier for the region.
                --reservation-affinity=: A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a DeployedModel) to draw its Compute Engine resources from a Shared Reservation, or exclusively from on-demand capacity.
                --spot: If true, schedule the deployment workload on Spot VM.
                --use-dedicated-endpoint: If true, the endpoint will be exposed through a dedicated DNS.
            - name: list
              description: List the publisher models in Model Garden.
              flags:
                --can-deploy-hugging-face-models: Whether to only list Hugging Face models that can be deployed.
                --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
                --full-resource-name: Whether to return the full resource name of the model.
                --limit=: Maximum number of resources to list.
                --model-filter=: Filter to apply to the model names or the display names of the list of models.
                --no-can-deploy-hugging-face-models&: Whether to only list Hugging Face models that can be deployed.
                --no-full-resource-name&: Whether to return the full resource name of the model.
                --page-size=: Some services group resource list output into pages.
                --sort-by=: Comma-separated list of resource field key names to sort by.
            - name: list-deployment-config
              description: List the machine specifications supported by and verified for a model in Model Garden.
              flags:
                --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
                --hugging-face-access-token=: The access token from Hugging Face needed to read the model artifacts of gated models in order to generate the deployment configs.
                --model=!: The model to be deployed.
                --sort-by=: Comma-separated list of resource field key names to sort by.
    - name: model-monitoring-jobs
      description: Manage Vertex AI model monitoring jobs.
      commands:
        - name: create
          description: Create a new Vertex AI model monitoring job.
          flags:
            --analysis-instance-schema=: YAML schema file uri(Google Cloud Storage) describing the format of a
            --anomaly-cloud-logging: If true, anomaly will be sent to Cloud Logging.
            --bigquery-uri=: BigQuery table of the unmanaged Dataset used to train this Model.
            --data-format=: Data format of the dataset, must be provided if the input is from Google Cloud Storage.
            --dataset=: Id of Vertex AI Dataset used to train this Model.
            --display-name=!: Display name of the model deployment monitoring job.
            --emails=!: Comma-separated email address list.
            --endpoint=!: Id of the endpoint.
            --feature-attribution-thresholds=: List of feature-attribution score threshold value pairs(Apply for all the
            --feature-thresholds=: List of feature-threshold value pairs(Apply for all the deployed models under
            --gcs-uris=: Comma-separated Google Cloud Storage uris of the unmanaged Datasets used to train this Model.
            --kms-key=!: ID of the key or fully qualified identifier for the key.
            --kms-keyring=: The KMS keyring of the key.
            --kms-location=: The Google Cloud location for the key.
            --kms-project=: The Google Cloud project for the key.
            --labels=: List of label KEY=VALUE pairs to add.
            --log-ttl=: TTL of BigQuery tables in user projects which stores logs(Day-based unit).
            --monitoring-config-from-file=: Path to the model monitoring objective config file.
            --monitoring-frequency=: Monitoring frequency, unit is 1 hour.
            --no-anomaly-cloud-logging&: If true, anomaly will be sent to Cloud Logging.
            --notification-channels=: Comma-separated notification channel list.
            --predict-instance-schema=: YAML schema file uri(Google Cloud Storage) describing the format of a
            --prediction-sampling-rate=!: Prediction sampling rate.
            --region=: ID of the region or fully qualified identifier for the region.
            --sample-predict-request=: Path to a local file containing the body of a JSON object.
            --target-field=: Target field name the model is to predict.
            --training-sampling-rate=: Training Dataset sampling rate.
        - name: delete
          description: Delete an existing Vertex AI model deployment monitoring job.
          flags:
            --region=: Cloud region for the monitoring_job.
        - name: describe
          description: Get detailed model deployment monitoring job information about the given job id.
          flags:
            --region=: Cloud region for the monitoring_job.
        - name: list
          description: List the model deployment monitoring jobs of the given project and region.
          flags:
            --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
            --limit=: Maximum number of resources to list.
            --no-uri&: Print a list of resource URIs instead of the default output, and change the
            --page-size=: Some services group resource list output into pages.
            --region=: ID of the region or fully qualified identifier for the region.
            --sort-by=: Comma-separated list of resource field key names to sort by.
            --uri: Print a list of resource URIs instead of the default output, and change the
        - name: pause
          description: Pause a running Vertex AI model deployment monitoring job.
          flags:
            --region=: Cloud region for the monitoring_job.
        - name: resume
          description: Resume a paused Vertex AI model deployment monitoring job.
          flags:
            --region=: Cloud region for the monitoring_job.
        - name: update
          description: Update an Vertex AI model deployment monitoring job.
          flags:
            --analysis-instance-schema=: YAML schema file uri(Google Cloud Storage) describing the format of a
            --anomaly-cloud-logging: If true, anomaly will be sent to Cloud Logging.
            --clear-labels: Remove all labels.
            --display-name=: Display name of the model deployment monitoring job.
            --emails=: Comma-separated email address list.
            --feature-attribution-thresholds=: List of feature-attribution score threshold value pairs(Apply for all the
            --feature-thresholds=: List of feature-threshold value pairs(Apply for all the deployed models under
            --log-ttl=: TTL of BigQuery tables in user projects which stores logs(Day-based unit).
            --monitoring-config-from-file=: Path to the model monitoring objective config file.
            --monitoring-frequency=: Monitoring frequency, unit is 1 hour.
            --no-anomaly-cloud-logging&: If true, anomaly will be sent to Cloud Logging.
            --no-clear-labels&: Remove all labels.
            --notification-channels=: Comma-separated notification channel list.
            --prediction-sampling-rate=: Prediction sampling rate.
            --region=: Cloud region for the monitoring_job.
            --remove-labels=: List of label keys to remove.
            --update-labels=: List of label KEY=VALUE pairs to update.
    - name: models
      description: Manage Vertex AI models.
      commands:
        - name: copy
          description: Copy a model.
          flags:
            --destination-model-id=: Copy source_model into a new Model with this ID.
            --destination-parent-model=: Specify this field to copy source_model into this existing Model as a new version.
            --kms-key-name=: The Cloud KMS resource identifier of the customer managed encryption key
            --region=: ID of the region or fully qualified identifier for the region.
            --source-model=!: The resource name of the Model to copy.
        - name: delete
          description: Delete an existing Vertex AI model.
          flags:
            --region=: Cloud region for the model.
        - name: delete-version
          description: Delete an existing Vertex AI model version.
          flags:
            --region=: Cloud region for the model.
        - name: describe
          description: Get detailed model information about the given model id.
          flags:
            --region=: Cloud region for the model.
        - name: list
          description: List the models of the given project and region.
          flags:
            --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
            --limit=: Maximum number of resources to list.
            --no-uri&: Print a list of resource URIs instead of the default output, and change the
            --page-size=: Some services group resource list output into pages.
            --region=: ID of the region or fully qualified identifier for the region.
            --sort-by=: Comma-separated list of resource field key names to sort by.
            --uri: Print a list of resource URIs instead of the default output, and change the
        - name: list-version
          description: List the model versions of the given region and model.
          flags:
            --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
            --limit=: Maximum number of resources to list.
            --no-uri&: Print a list of resource URIs instead of the default output, and change the
            --page-size=: Some services group resource list output into pages.
            --region=: Cloud region for the model.
            --sort-by=: Comma-separated list of resource field key names to sort by.
            --uri: Print a list of resource URIs instead of the default output, and change the
        - name: upload
          description: Upload a new model.
          flags:
            --artifact-uri=: Path to the directory containing the Model artifact and any of its
            --container-args=: Comma-separated arguments passed to the command run by the container
            --container-command=: Entrypoint for the container image.
            --container-deployment-timeout-seconds=: Deployment timeout in seconds.
            --container-env-vars=: List of key-value pairs to set as environment variables.
            --container-grpc-ports=: Container ports to receive grpc requests at.
            --container-health-probe-exec=: Exec specifies the action to take.
            --container-health-probe-period-seconds=: How often (in seconds) to perform the health probe.
            --container-health-probe-timeout-seconds=: Number of seconds after which the health probe times out.
            --container-health-route=: HTTP path to send health checks to inside the container.
            --container-image-uri=!: URI of the Model serving container file in the Container Registry
            --container-ports=: Container ports to receive http requests at.
            --container-predict-route=: HTTP path to send prediction requests to inside the container.
            --container-shared-memory-size-mb=: The amount of the VM memory to reserve as the shared memory for the model in
            --container-startup-probe-exec=: Exec specifies the action to take.
            --container-startup-probe-period-seconds=: How often (in seconds) to perform the startup probe.
            --container-startup-probe-timeout-seconds=: Number of seconds after which the startup probe times out.
            --description=: Description of the model.
            --display-name=!: Display name of the model.
            --explanation-metadata-file=: Path to a local JSON file that contains the metadata describing the Model's input and output for explanation.
            --explanation-method=: Method used for explanation.
            --explanation-path-count=: Number of feature permutations to consider when approximating the Shapley values for explanation.
            --explanation-step-count=: Number of steps to approximate the path integral for explanation.
            --labels=: Labels with user-defined metadata to organize your Models.
            --model-id=: ID to use for the uploaded Model, which will become the final component of the model resource name.
            --parent-model=: Resource name of the model into which to upload the version.
            --region=: ID of the region or fully qualified identifier for the region.
            --smooth-grad-noise-sigma-by-feature=: Noise sigma by features for explanation.
            --smooth-grad-noise-sigma=: Single float value used to add noise to all the features for explanation.
            --smooth-grad-noisy-sample-count=: Number of gradient samples used for approximation at explanation.
            --version-aliases=: Aliases used to reference a model version instead of auto-generated version ID.
            --version-description=: Description of the model version.
    - name: operations
      description: Manage Vertex AI operations.
      commands:
        - name: describe
          description: Gets detailed index information about the given operation id.
          flags:
            --index-endpoint=: ID of the index endpoint.
            --index=: ID of the index.
            --region=: Cloud region for the operation.
    - name: persistent-resources
      description: Create and manage Vertex AI persistent resources.
      commands:
        - name: create
          description: Create a new persistent resource.
          flags:
            --config=: Path to the Persistent Resource configuration file.
            --display-name=: Display name of the Persistent Resource.
            --enable-custom-service-account: Whether or not to use a custom user-managed service account with this
            --kms-key=!: ID of the key or fully qualified identifier for the key.
            --kms-keyring=: The KMS keyring of the key.
            --kms-location=: The Google Cloud location for the key.
            --kms-project=: The Google Cloud project for the key.
            --labels=: List of label KEY=VALUE pairs to add.
            --network=: Full name of the Google Compute Engine network to which the Job
            --no-enable-custom-service-account&: Whether or not to use a custom user-managed service account with this
            --persistent-resource-id=!: User-specified ID of the Persistent Resource.
            --region=: ID of the region or fully qualified identifier for the region.
            --resource-pool-spec=: Defines a resource pool to be created in the Persistent Resource.
        - name: delete
          description: Delete an active Persistent Resource.
          flags:
            --region=: Cloud region for the persistent resource.
        - name: describe
          description: Get detailed information about a PersistentResource with a given id.
          flags:
            --region=: Cloud region for the persistent resource.
        - name: list
          description: Lists the active persistent resources.
          flags:
            --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
            --limit=: Maximum number of resources to list.
            --no-uri&: Print a list of resource URIs instead of the default output, and change the
            --page-size=: Some services group resource list output into pages.
            --region=: ID of the region or fully qualified identifier for the region.
            --sort-by=: Comma-separated list of resource field key names to sort by.
            --uri: Print a list of resource URIs instead of the default output, and change the
        - name: reboot
          description: Reboot a Persistent Resource.
          flags:
            --region=: Cloud region for the persistent resource.
    - name: tensorboards
      description: Manage Vertex AI Tensorboards.
      commands:
        - name: create
          description: Create a new Vertex AI Tensorboard.
          flags:
            --description=: Description of the tensorboard.
            --display-name=!: Display name of the tensorboard.
            --kms-key=!: ID of the key or fully qualified identifier for the key.
            --kms-keyring=: The KMS keyring of the key.
            --kms-location=: The Google Cloud location for the key.
            --kms-project=: The Google Cloud project for the key.
            --labels=: List of label KEY=VALUE pairs to add.
            --region=: ID of the region or fully qualified identifier for the region.
        - name: delete
          description: Delete an existing Vertex AI Tensorboard.
          flags:
            --region=: Cloud region for the tensorboard.
        - name: describe
          description: Gets detailed Tensorboard information about the given Tensorboard id.
          flags:
            --region=: Cloud region for the tensorboard.
        - name: list
          description: Lists the Tensorboards of the given project and region.
          flags:
            --filter=: Apply a Boolean filter _EXPRESSION_ to each resource item to be listed.
            --limit=: Maximum number of resources to list.
            --no-uri&: Print a list of resource URIs instead of the default output, and change the
            --page-size=: Some services group resource list output into pages.
            --region=: ID of the region or fully qualified identifier for the region.
            --sort-by=: Comma-separated list of resource field key names to sort by.
            --uri: Print a list of resource URIs instead of the default output, and change the
        - name: update
          description: Update an existing Vertex AI Tensorboard.
          flags:
            --clear-labels: Remove all labels.
            --description=: Description of the tensorboard.
            --display-name=: Display name of the tensorboard.
            --no-clear-labels&: Remove all labels.
            --region=: Cloud region for the tensorboard.
            --remove-labels=: List of label keys to remove.
            --update-labels=: List of label KEY=VALUE pairs to update.
